{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7xlSLIDby0YIKV3VZvaUO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirnaderiy/CNN_ViT/blob/main/ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python\n",
        "!pip install efficientnet\n",
        "!pip install tensorflow_addons\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYSnfqUpF8G0",
        "outputId": "8b205fee-63f6-4630-8558-414616023730"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from opencv-python) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: efficientnet in /usr/local/lib/python3.9/dist-packages (1.1.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.9/dist-packages (from efficientnet) (0.19.3)\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.9/dist-packages (from efficientnet) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.22.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image->efficientnet) (23.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->efficientnet) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image->efficientnet) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->efficientnet) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image->efficientnet) (2023.4.12)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image->efficientnet) (8.4.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->efficientnet) (1.4.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.9/dist-packages (0.20.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.9/dist-packages (from tensorflow_addons) (2.13.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow_addons) (23.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "import tensorflow as tf\n",
        "import os , shutil\n",
        "import cv2\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import LearningRateScheduler, LambdaCallback, EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n"
      ],
      "metadata": {
        "id": "g5GcJu8gkZMQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWEjNB1yHG1Q",
        "outputId": "02eea4ae-3323-4c6e-8d3f-fef624c66cb8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VoPalHgWj-AP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "b0939b06-eb2c-4396-d2ed-76abec2a7193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "25/25 [==============================] - 88s 3s/step - loss: 2.9945 - accuracy: 0.3938 - precision: 0.4532 - recall: 0.3264 - val_loss: 24.1778 - val_accuracy: 0.1813 - val_precision: 0.1813 - val_recall: 0.1813\n",
            "Epoch 2/250\n",
            "25/25 [==============================] - 61s 2s/step - loss: 2.5298 - accuracy: 0.4741 - precision: 0.5433 - recall: 0.3575 - val_loss: 13.3533 - val_accuracy: 0.1813 - val_precision: 0.1813 - val_recall: 0.1813\n",
            "Epoch 3/250\n",
            "25/25 [==============================] - 62s 3s/step - loss: 2.3605 - accuracy: 0.5259 - precision: 0.5993 - recall: 0.4301 - val_loss: 10.1443 - val_accuracy: 0.1865 - val_precision: 0.1875 - val_recall: 0.1865\n",
            "Epoch 4/250\n",
            "25/25 [==============================] - 62s 3s/step - loss: 2.2336 - accuracy: 0.5570 - precision: 0.6099 - recall: 0.4456 - val_loss: 6.4609 - val_accuracy: 0.1865 - val_precision: 0.1875 - val_recall: 0.1865\n",
            "Epoch 5/250\n",
            "25/25 [==============================] - 63s 3s/step - loss: 2.0768 - accuracy: 0.5518 - precision: 0.6160 - recall: 0.4197 - val_loss: 7.8850 - val_accuracy: 0.2124 - val_precision: 0.2124 - val_recall: 0.2124\n",
            "Epoch 6/250\n",
            "25/25 [==============================] - 62s 3s/step - loss: 1.9657 - accuracy: 0.5907 - precision: 0.6522 - recall: 0.5052 - val_loss: 7.9561 - val_accuracy: 0.1606 - val_precision: 0.1623 - val_recall: 0.1606\n",
            "Epoch 7/250\n",
            "25/25 [==============================] - 61s 2s/step - loss: 1.9942 - accuracy: 0.5466 - precision: 0.6157 - recall: 0.4482 - val_loss: 10.0518 - val_accuracy: 0.1658 - val_precision: 0.1658 - val_recall: 0.1658\n",
            "Epoch 8/250\n",
            "25/25 [==============================] - 62s 2s/step - loss: 1.8115 - accuracy: 0.6010 - precision: 0.6497 - recall: 0.4948 - val_loss: 8.0897 - val_accuracy: 0.1762 - val_precision: 0.1762 - val_recall: 0.1762\n",
            "Epoch 9/250\n",
            "25/25 [==============================] - 62s 2s/step - loss: 1.9102 - accuracy: 0.5518 - precision: 0.6154 - recall: 0.4767 - val_loss: 7.2111 - val_accuracy: 0.2176 - val_precision: 0.2176 - val_recall: 0.2176\n",
            "13/13 [==============================] - 20s 2s/step - loss: 6.3163 - accuracy: 0.1865 - precision: 0.1885 - recall: 0.1865\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e0740d6f80b9>\u001b[0m in \u001b[0;36m<cell line: 149>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m                     callbacks=[early_stopping])\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "original_dataset_dir = '/content/drive/MyDrive/B-Mode Ultrasound/'\n",
        "\n",
        "\n",
        "# Define the four groups based on the filename\n",
        "group0 = [filename for filename in os.listdir(original_dataset_dir) if filename.startswith('Group0')]\n",
        "group1 = [filename for filename in os.listdir(original_dataset_dir) if filename.startswith('Group1')]\n",
        "group2 = [filename for filename in os.listdir(original_dataset_dir) if filename.startswith('Group2')]\n",
        "group3 = [filename for filename in os.listdir(original_dataset_dir) if filename.startswith('Group3')]\n",
        "\n",
        "# Define a function to load the images and resize them to a specific size\n",
        "def load_and_resize_image(filename, target_size):\n",
        "    img = cv2.imread(os.path.join(original_dataset_dir, filename))\n",
        "    img = cv2.resize(img, target_size)\n",
        "    return img\n",
        "\n",
        "# Define the target size for the images\n",
        "target_size = (224, 224)\n",
        "\n",
        "# Load the images for each group and resize them\n",
        "group0_images = [load_and_resize_image(filename, target_size) for filename in group0]\n",
        "group1_images = [load_and_resize_image(filename, target_size) for filename in group1]\n",
        "group2_images = [load_and_resize_image(filename, target_size) for filename in group2]\n",
        "group3_images = [load_and_resize_image(filename, target_size) for filename in group3]\n",
        "\n",
        "# Define the labels for each group\n",
        "group0_labels = [0] * len(group0_images)\n",
        "group1_labels = [1] * len(group1_images)\n",
        "group2_labels = [2] * len(group2_images)\n",
        "group3_labels = [3] * len(group3_images)\n",
        "\n",
        "images = group0_images + group1_images + group2_images + group3_images\n",
        "labels = group0_labels + group1_labels + group2_labels + group3_labels\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.30, random_state=42)\n",
        "\n",
        "# Split the train set into train and validation sets\n",
        "test_images, val_images, test_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.50, random_state=42)\n",
        "\n",
        "target_size = (384, 384)\n",
        "\n",
        "# Load the images for each group and resize them\n",
        "group0_images = [load_and_resize_image(filename, target_size) for filename in group0]\n",
        "group1_images = [load_and_resize_image(filename, target_size) for filename in group1]\n",
        "group2_images = [load_and_resize_image(filename, target_size) for filename in group2]\n",
        "group3_images = [load_and_resize_image(filename, target_size) for filename in group3]\n",
        "\n",
        "# Define the labels for each group\n",
        "group0_labels = [0] * len(group0_images)\n",
        "group1_labels = [1] * len(group1_images)\n",
        "group2_labels = [2] * len(group2_images)\n",
        "group3_labels = [3] * len(group3_images)\n",
        "\n",
        "images = group0_images + group1_images + group2_images + group3_images\n",
        "labels = group0_labels + group1_labels + group2_labels + group3_labels\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.30, random_state=42)\n",
        "\n",
        "# Split the train set into train and validation sets\n",
        "test_images, val_images, test_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.50, random_state=42)\n",
        "\n",
        "    \n",
        "image_size = 384\n",
        "\n",
        "# Define the number of classes\n",
        "num_classes=4\n",
        "\n",
        "# Load the Swin Transformer model\n",
        "swin = hub.KerasLayer(\"https://tfhub.dev/sayakpaul/swin_large_patch4_window12_384_in22k/1\")\n",
        "\n",
        "\n",
        "# Add your own fully connected layers\n",
        "model = models.Sequential()\n",
        "model.add(swin)\n",
        "model.add(layers.Dense(512, kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Define the batch size for training and validation\n",
        "batch_size = 16\n",
        "\n",
        "# Define the data generators for training, validation, and testing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow(\n",
        "    x=np.array(train_images),\n",
        "    y=tf.keras.utils.to_categorical(train_labels, num_classes=num_classes),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow(\n",
        "    x=np.array(val_images),\n",
        "    y=tf.keras.utils.to_categorical(val_labels, num_classes=num_classes),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow(\n",
        "    x=np.array(test_images),\n",
        "    y=tf.keras.utils.to_categorical(test_labels, num_classes=num_classes),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', \n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "    \n",
        "history = model.fit(train_generator, \n",
        "                    steps_per_epoch=len(train_generator), \n",
        "                    epochs=250, \n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=len(validation_generator),\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator, steps=len(test_generator))\n",
        "\n",
        "history_dict=history.history\n",
        "accuracy_values=history_dict['accuracy']\n",
        "val_accuracy_values=history_dict['val_accuracy']\n",
        "test_accuracy_values = [test_acc] * len(accuracy_values)\n",
        "epochs=range(1,len(accuracy_values)+1)\n",
        "plt.plot (epochs,accuracy_values,'bo',label='Training Accuracy')\n",
        "plt.plot (epochs,val_accuracy_values,'r',label='Validation Accuracy')\n",
        "plt.plot (epochs,test_accuracy_values,'g',label='Test Accuracy')\n",
        "plt.title('Training, Validation and Test Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "accuracy_values=history_dict['loss']\n",
        "val_accuracy_values=history_dict['val_loss']\n",
        "test_loss_values = [test_loss] * len(accuracy_values)\n",
        "epochs=range(1,len(accuracy_values)+1)\n",
        "plt.plot (epochs,accuracy_values,'b*',label='Training Loss')\n",
        "plt.plot (epochs,val_accuracy_values,'r',label='Validation Loss')\n",
        "plt.plot (epochs,test_loss_values,'g',label='Test Loss')\n",
        "plt.title('Training, Validation and Test Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict=history.history\n",
        "accuracy_values=history_dict['accuracy']\n",
        "val_accuracy_values=history_dict['val_accuracy']\n",
        "test_accuracy_values = [test_acc] * len(accuracy_values)\n",
        "epochs=range(1,len(accuracy_values)+1)\n",
        "plt.plot (epochs,accuracy_values,'bo',label='Training Accuracy')\n",
        "plt.plot (epochs,val_accuracy_values,'r',label='Validation Accuracy')\n",
        "plt.plot (epochs,test_accuracy_values,'g',label='Test Accuracy')\n",
        "plt.title('Training, Validation and Test Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "accuracy_values=history_dict['loss']\n",
        "val_accuracy_values=history_dict['val_loss']\n",
        "test_loss_values = [test_loss] * len(accuracy_values)\n",
        "epochs=range(1,len(accuracy_values)+1)\n",
        "plt.plot (epochs,accuracy_values,'b*',label='Training Loss')\n",
        "plt.plot (epochs,val_accuracy_values,'r',label='Validation Loss')\n",
        "plt.plot (epochs,test_loss_values,'g',label='Test Loss')\n",
        "plt.title('Training, Validation and Test Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1DIspujGkT6l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}